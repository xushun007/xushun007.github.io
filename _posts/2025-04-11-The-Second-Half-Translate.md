---
title: 【译】AI 下半场
author: Shunyu Yao
date: 2025-04-10
categories: [AI]
tags: [AI]
render_with_liquid: false
---


### **下半场**

**核心摘要：我们正处于人工智能的中场休息时刻。**

几十年来，人工智能（AI）主要致力于开发新的训练方法和模型。这一策略卓有成效：从在国际象棋和围棋中击败世界冠军，到在SAT和律师资格考试中超越大多数人类，再到斩获国际数学奥林匹克（IMO）和国际信息学奥林匹克（IOI）金牌。在这些载入史册的里程碑——深蓝（DeepBlue）、阿尔法围棋（AlphaGo）、GPT-4以及o系列模型——背后，是AI方法论的根本性创新：搜索、深度强化学习（deep RL）、规模化（scaling）和推理（reasoning）。一切都随着时间的推移而变得更好。

那么，现在究竟发生了什么突变？

三个词足以概括：**强化学习（RL）终于奏效了**。更准确地说：**强化学习终于具备了泛化能力**。在经历了数次重大的弯路和一系列里程碑式的积累之后，我们终于找到了一套行之有效的“秘方”，能够利用语言和推理解决广泛的强化学习任务。即便在一年前，如果你告诉大多数AI研究者，同一个“秘方”就能搞定软件工程、创意写作、IMO级别的数学、键鼠操作以及长篇问答——他们会嘲笑你是在痴人说梦。这些任务中的每一个都极其困难，许多研究者穷其整个博士生涯也只专注于其中一个狭窄的领域。

然而，这一切真的发生了。

那么接下来会发生什么？AI的“下半场”——从现在开始——将把焦点从**解决问题**转向**定义问题**。在这个新时代，**评估（evaluation）变得比训练（training）更重要**。我们不再仅仅问：“我们能训练一个模型来解决X问题吗？”，而是要问：“我们应该训练AI去做什么，以及我们如何衡量真正的进展？” 要想在“下半场”脱颖而出，我们需要及时转变思维模式和技能组合，或许要向产品经理的角色靠拢。

#### **上半场**

要理解“上半场”，只需看看它的赢家。你认为迄今为止最具影响力的AI论文是哪些？

我试着做了斯坦福大学CS224N课程里的小测验，答案不出所料：Transformer、AlexNet、GPT-3等等。这些论文有何共同之处？它们都提出了一些根本性的突破，用以训练出更好的模型。同时，它们也通过在某些基准测试上展示出（显著的）提升而成功发表。

不过，这里还有一个潜在的共性：这些“赢家”都是**训练方法或模型**，而非**基准测试或任务**。即便是公认最具影响力的基准测试ImageNet，其引用量也不到AlexNet的三分之一。方法与基准测试之间的反差在其他地方更为悬殊——例如，Transformer模型的主要基准是WMT’14，其研讨会报告的引用量约为1300次，而Transformer论文的引用量超过了16万次。

![](https://ysymyth.github.io/images/second_half/first_half.png)

这清晰地描绘了“上半场”的游戏规则：专注于构建新的模型和方法，而评估和基准测试则是次要的（尽管对于论文发表体系的运作是必需的）。

为什么会这样？一个重要原因是，在AI的“上半场”，**方法比任务更困难，也更激动人心**。从零开始创造一种新的算法或模型架构——比如反向传播算法、卷积网络（AlexNet）或GPT-3中使用的Transformer——需要非凡的洞察力和工程能力。相比之下，为AI定义任务通常感觉更直接：我们只是把人类已经在做的任务（如翻译、图像识别或下棋）变成基准测试。这其中并不需要太多洞察力，甚至工程量也不大。

方法也往往比单个任务更通用、适用范围更广，这使其价值尤为突出。例如，Transformer架构最终推动了计算机视觉（CV）、自然语言处理（NLP）、强化学习（RL）等众多领域的进步——远远超出了它最初证明自己的那个单一数据集（WMT’14翻译任务）。一个优秀的新方法之所以能攻克多个不同的基准测试，是因为它简洁而通用，因此其影响力往往超越单个任务。

这个游戏规则运行了几十年，催生了改变世界的想法和突破，这些成果体现在各个领域基准测试性能的不断提升上。那为什么这个游戏规则会发生改变呢？因为这些想法和突破的累积，在创造一套解决任务的有效“秘方”方面，已经引发了质变。

#### **那套“秘方”**

这套“秘方”是什么？不出所料，其配料包括：海量语言预训练、规模化（数据和算力层面），以及推理与行动的理念。这些听起来可能像你在旧金山每天都能听到的时髦词汇，但为什么称之为“秘方”呢？

我们可以通过强化学习（RL）的视角来理解这一点。RL常被视为AI的“终局之战”——毕竟，从理论上讲，RL保证能赢得游戏；从经验上看，也很难想象任何超人系统（如AlphaGo）不使用RL。

在RL中，有三个关键组成部分：**算法（algorithm）**、**环境（environment）**和**先验知识（priors）**。长期以来，RL研究者主要关注算法（例如REINFORCE、DQN、TD-learning、actor-critic、PPO、TRPO……）——这是智能体学习方式的智力核心——而将环境和先验知识视为固定或最简化的。例如，Sutton和Barto的经典教科书通篇都在讲算法，几乎没有涉及环境或先验知识。

![](https://ysymyth.github.io/images/second_half/rl_book.png)

然而，在深度强化学习时代，环境在经验层面上的重要性变得显而易见：一个算法的性能往往高度依赖于其开发和测试的环境。如果忽略环境，你可能会构建出一个只在“玩具”环境中表现出色的“最优”算法。那么，我们为什么不先弄清楚我们真正想解决的环境，然后再找到最适合该环境的算法呢？

这正是OpenAI最初的计划。它创建了`gym`，一个包含各种游戏的标准RL环境，然后又发起了`World of Bits`和`Universe`项目，试图将整个互联网或计算机变成一个游戏。这计划听起来不错，不是吗？一旦我们将所有数字世界都变成一个环境，再用聪明的RL算法解决它，我们就拥有了数字化的通用人工智能（AGI）。

计划虽好，但并未完全成功。OpenAI沿着这条路取得了巨大进展，用RL解决了《Dota》、机械手控制等问题。但它从未接近解决计算机通用操作或网页导航的问题，而且在一个领域有效的RL智能体也无法迁移到另一个领域。**有什么东西缺失了。**

直到GPT-2或GPT-3之后，人们才发现，**缺失的那一块是先验知识**。你需要强大的语言预训练来将通用的常识和语言知识提炼到模型中，然后这些模型才能通过微调成为网页智能体（WebGPT）或聊天智能体（ChatGPT）（并改变世界）。事实证明，**RL最重要的部分甚至可能不是RL算法或环境，而是先验知识**，而这些先验知识的获取方式可以与RL完全无关。

语言预训练为聊天任务创造了良好的先验知识，但对于控制计算机或玩视频游戏的效果却没那么好。为什么？因为这些领域与互联网文本的分布相去甚远，在这些领域上简单地进行监督微调（SFT）或强化学习（RL），其泛化能力很差。我在2019年就注意到了这个问题，当时GPT-2刚发布，我基于它进行SFT/RL来解决文字冒险游戏——我开发的`CALM`是世界上第一个基于预训练语言模型构建的智能体。但它需要数百万步的RL训练才能攻克一个游戏，并且无法迁移到新游戏上。尽管这完全是RL的特性，对RL研究者来说不足为奇，但我却觉得很奇怪，因为我们人类可以轻松地玩一个新游戏，并且在零样本（zero-shot）情况下表现得好得多。然后，我迎来了人生中第一个“尤里卡时刻”——我们之所以能够泛化，是因为我们可以选择做的不仅仅是“去2号柜子”、“用1号钥匙打开3号宝箱”或“用剑杀死地牢里的怪物”，我们还可以选择**思考**，比如：“地牢很危险，我需要一把武器来战斗。这里没有看得见的武器，也许我需要到上锁的箱子里找找看。3号宝箱在2号柜子里，让我先去那里把它解锁。”

![](https://ysymyth.github.io/images/second_half/reasoning.png)

**思考，或者说推理，是一种奇特的“行动”**——它不直接影响外部世界，但推理的空间是开放且组合爆炸的——你可以想一个词、一个句子、一整段话，或者10000个随机的英文单词，而你周围的世界并不会立即改变。在经典的RL理论中，这是一笔糟糕的交易，让决策变得不可能。想象一下，你需要在两个箱子中选一个，其中一个有100万美元，另一个是空的。你的期望收益是50万美元。现在想象我加入了无限个空箱子，你的期望收益就变成了零。但是，通过将推理加入任何RL环境的行动空间，我们利用了语言预训练的先验知识来实现泛化，并且我们有能力在测试时为不同的决策分配灵活的计算资源。这真的是一件非常**神奇**的事情，很抱歉我在这里没能完全解释清楚，可能需要另写一篇博客来专门阐述。欢迎大家阅读`ReAct`论文，了解关于智能体推理的原始故事，并感受我当时的一些想法。目前，我的直观解释是：**尽管你增加了无限个空箱子，但你在各种游戏中一生都在看到它们，选择这些箱子能让你准备得更好，以便在任何给定的游戏中选择那个有钱的箱子。** 我的抽象解释是：**语言通过智能体中的推理来实现泛化。**

一旦我们有了正确的RL先验知识（语言预训练）和RL环境（将语言推理作为行动加入），结果发现RL算法可能是最微不足道的部分。因此，我们看到了o系列模型、R1、深度研究、计算机操作智能体以及未来更多的成果。这是多么具有讽刺意味的转折！长期以来，RL研究者对算法的关心远超环境，而几乎没人关注先验知识——所有RL实验基本上都是从零开始的。但我们花了几十年的弯路才意识到，**也许我们优先级的排序本应完全颠倒**。

但正如史蒂夫·乔布斯所说：你无法预见性地将点滴串联起来；只有在回顾时，你才能将它们联系起来。

#### **下半场**

这个“秘方”正在彻底改变游戏规则。回顾一下“上半场”的游戏：
1. 我们开发新颖的训练方法或模型来攻克基准测试。
2. 我们创造更难的基准测试，然后继续这个循环。

这个游戏正在被颠覆，因为：
1. 这套“秘方”已经将攻克基准测试的过程**标准化和工业化**了，不再需要太多新思想。随着这套“秘方”的规模和泛化能力越来越强，你针对某个特定任务提出的新方法可能只能带来5%的提升，而下一个o系列模型无需专门针对该任务就能带来30%的提升。
2. 即使我们创造出更难的基准测试，它们很快（而且越来越快地）就会被这套“秘方”解决。我的同事Jason Wei制作了一张精美的图表，很好地展示了这一趋势：

![](https://ysymyth.github.io/images/second_half/progress.jpeg)

那么，在“下半场”还有什么可玩的？如果新颖的方法不再必要，更难的基准也只会被越来越快地解决，我们该做什么？

我认为，**我们应该从根本上重新思考评估**。这不仅仅意味着创造新的、更难的基准，而是要从根本上**质疑现有的评估范式（evaluation setups）**，并创造新的范式，从而迫使我们去发明超越现有“秘方”的新方法。这很困难，因为人类有惯性，很少质疑基本假设——你只是把它们当作理所当然，而没有意识到它们是假设，而非定律。

为了解释惯性，假设你基于人类考试发明了历史上最成功的评估方法之一。这在2021年是一个极其大胆的想法，但3年后它就饱和了。你会怎么做？很可能去创造一个**难得多的考试**。或者假设你解决了**简单的编程任务**。你会怎么做？很可能去找**更难的编程任务**来解决，直到达到IOI金牌水平。

惯性是自然的，但问题在于：AI已经在国际象棋和围棋中击败了世界冠军，在SAT和律师资格考试中超越了大多数人类，并在IOI和IMO中达到了金牌水平。**但世界并没有因此发生太大变化**，至少从经济和GDP来看是这样。

我称之为**效用问题（utility problem）**，并认为这是AI最重要的问题。

也许我们很快就能解决这个效用问题，也许不能。无论如何，这个问题的根源可能非常简单：**我们的评估范式在许多基本方面与真实世界场景不同**。举两个例子：
- 评估“理应”自动运行，所以通常一个智能体接收任务输入，自主完成任务，然后获得任务奖励。但在现实中，一个智能体在整个任务过程中必须与人互动——你不会只给客服发一条超长信息，等10分钟，然后指望一个详尽的回复就解决所有问题。通过质疑这种范式，新的基准测试被发明出来，将**真人**（如`Chatbot Arena`）或**用户模拟**（如`tau-bench`）纳入评估环路。

![](https://ysymyth.github.io/images/second_half/tau.png)

- 评估“理应”以**独立同分布（i.i.d.）**的方式运行。如果你有一个包含500个任务的测试集，你会独立运行每个任务，然后对任务指标取平均，得到一个总体指标。但在现实中，你是**按顺序**解决任务，而不是并行。一位谷歌的软件工程师随着对代码库越来越熟悉，解决问题的效率会越来越高，但一个软件工程师智能体在同一个代码库中解决多个问题时，却不会获得这种熟悉度。我们显然需要长期记忆方法（而且已经有了），但学术界没有合适的基准来证明其必要性，甚至没有足够的勇气去质疑这个作为机器学习基础的独立同分布假设。

这些假设“一直”就是这样，在AI的“上半场”，在这些假设下开发基准是没问题的，因为**当智能水平较低时，提升智能通常也能提升效用**。但现在，那套通用的“秘方”在这些假设下保证能行得通。所以，“下半场”的新游戏规则是：
1. 我们为**真实世界的效用**开发新颖的评估范式或任务。
2. 我们用现有的“秘方”解决它们，或者为“秘方”增加新的组件来解决。然后继续这个循环。

这个游戏很难，因为它不为我们所熟悉。但它激动人心。当“上半场”的玩家在解决视频游戏和考试时，“下半场”的玩家将通过利用智能构建有用的产品，从而创建价值数十亿甚至万亿美元的公司。当“上半场”充满了渐进式的方法和模型时，“下半场”在某种程度上会筛选掉它们。通用的“秘方”会碾压你的渐进式方法，除非你创造出能打破这套“秘方”的新假设。那时，你才能做出真正改变游戏规则的研究。

欢迎来到下半场！

**致谢**
这篇博文基于我在斯坦福CS224N课程和哥伦比亚大学的演讲。我使用了OpenAI的深度研究工具来阅读我的幻灯片并撰写初稿。

写于 2025年4月10日

原文：[The Second Half](https://ysymyth.github.io/The-Second-Half/)

---

### **【非原文内容】学习总结**

这篇文章的核心论点是，人工智能的发展已经进入了一个全新的阶段，作者称之为“下半场”，其游戏规则与“上半场”截然不同。

1.  **“上半场”的回顾**：过去几十年的AI发展主要聚焦于**创造新的训练方法和模型**（如Transformer、AlexNet），并通过在固定的基准测试（如ImageNet）上取得更高分数来衡量进展。这一模式取得了巨大成功。

2.  **转折点——“秘方”的出现**：如今，一个由**大规模语言预训练、规模化和推理能力**组成的通用“秘方”已经形成。这个“秘方”使得强化学习（RL）真正具备了泛化能力，能够高效解决各类过去被认为是独立且困难的任务（从编程到数学）。其关键在于认识到“先验知识”（来自语言模型）和将“推理”作为一种行动的重要性，这甚至比算法本身更重要。

3.  **“下半场”的新规则**：由于这个强大的“秘方”能轻易攻克现有基准，继续创造更难的基准已经意义不大。“下半场”的焦点必须从“如何解决问题”转向**“应该定义什么问题”**。核心任务是**从根本上重新思考评估（Evaluation）**，创造出能反映**真实世界效用（Real-world Utility）**的新评估范式。

4.  **核心挑战——“效用问题”**：作者指出，尽管AI在考试、游戏等任务上已达超人水平，但其对现实世界经济（如GDP）的改变有限。根源在于当前的评估范式（如任务自动化、独立同分布假设）与真实世界的人机协作、持续学习场景严重脱节。

5.  **未来方向**：未来的突破将属于那些能够**定义出更有价值、更贴近现实场景的问题和评估方式**的人。这不仅能催生出真正有用的产品和万亿级公司，也将迫使研究者们跳出现有“秘方”的框架，进行真正具有颠覆性的创新。


